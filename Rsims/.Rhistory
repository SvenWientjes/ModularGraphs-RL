# Get the hitMat
hitMat <- foreach(v=inspect.Vertices, .combine=rbind) %do% {
tempMat <- MC.hitMat(Edges=Edges, vStart=v, vGoal=vGoal, nSteps=nSteps-1, nSamp=10000)
tempMat
}
hitMat
# Get the hitMat
hitMat <- foreach(v=inspect.Vertices, .combine=rbind) %do% {
tempMat <- MC.hitMat(Edges=Edges, vStart=v, vGoal=vGoal, nSteps=nSteps-1, nSamp=50000)
tempMat
}
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
hitMat
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
tempMat
remove(tempMat)
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
debug(EVcalc)
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
EVdat
intRew
intRew
hitMat
hitMat[hitMat$vertex==Vertex,]
Vertex
hitMat
hitMat
names(hitMat)
names(hitMat)[1]
names(hitMat)[1] <- 'vertex'
hitMat
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
undebug(EVcalc)
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
EVmat
piMat <- policy.generate(Edges=Edges, EVmat=EVmat, idmap=idmap)
piMat
sCost  <- 0.1 # Points detracted from accumulated reward for each taken step
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
piMat <- policy.generate(Edges=Edges, EVmat=EVmat, idmap=idmap)
piMat
# Run several agents on this task, all encompassing different heuristics
AG.dat <- data.frame(pp=0, trial=0, trRew=0, nSteps=0, endV=0, totRew=0, strat='init')
for(pp in 1:nPP){
AG.dat <- rbind(AG.dat, RandomStopper( Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, LazyWaiter(    Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, RandomLengther(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, OptimalStopper(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0, piMat=piMat))
}
AG.dat <- AG.dat[-1,]
AG.dat$strat <- droplevels(AG.dat$strat)
# Summarize agent simulations per participant
AG.dat.ppEval <- data.frame(pp=0, totRew=0, endV.p=0, strat='init')
for(pp in 1:nPP){
for(strat in levels(AG.dat$strat)){
totRew <- AG.dat[AG.dat$pp==pp & AG.dat$trial==max(AG.dat$trial) & AG.dat$strat==strat,]$totRew
endV.p <- sum(AG.dat[AG.dat$pp==pp & AG.dat$strat==strat,]$endV==vGoal)
AG.dat.ppEval <- rbind(AG.dat.ppEval, data.frame(pp=pp, totRew=totRew, endV.p=endV.p, strat=strat))
}
}
AG.dat.ppEval <- AG.dat.ppEval[-1,]
# Plot agent data!
ggplot(AG.dat.ppEval, aes(x=totRew, col=strat)) +
geom_density()
# Get the hitMat
hitMat <- foreach(v=inspect.Vertices, .combine=rbind) %do% {
tempMat <- MC.hitMat(Edges=Edges, vStart=v, vGoal=vGoal, nSteps=nSteps-1, nSamp=50000)
tempMat
}
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
piMat <- policy.generate(Edges=Edges, EVmat=EVmat, idmap=idmap)
# Run several agents on this task, all encompassing different heuristics
AG.dat <- data.frame(pp=0, trial=0, trRew=0, nSteps=0, endV=0, totRew=0, strat='init')
for(pp in 1:nPP){
AG.dat <- rbind(AG.dat, RandomStopper( Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, LazyWaiter(    Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, RandomLengther(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, OptimalStopper(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0, piMat=piMat))
}
AG.dat <- AG.dat[-1,]
AG.dat$strat <- droplevels(AG.dat$strat)
# Summarize agent simulations per participant
AG.dat.ppEval <- data.frame(pp=0, totRew=0, endV.p=0, strat='init')
for(pp in 1:nPP){
for(strat in levels(AG.dat$strat)){
totRew <- AG.dat[AG.dat$pp==pp & AG.dat$trial==max(AG.dat$trial) & AG.dat$strat==strat,]$totRew
endV.p <- sum(AG.dat[AG.dat$pp==pp & AG.dat$strat==strat,]$endV==vGoal)
AG.dat.ppEval <- rbind(AG.dat.ppEval, data.frame(pp=pp, totRew=totRew, endV.p=endV.p, strat=strat))
}
}
AG.dat.ppEval <- AG.dat.ppEval[-1,]
# Plot agent data!
ggplot(AG.dat.ppEval, aes(x=totRew, col=strat)) +
geom_density()
# Function that return posterior mean for sampled paths over graph
MC.hitMat <- function(Edges, vStart, vGoal, nSteps, nSamp){
MCounter <- rep(0, nSteps+1)
for(r in 1:nSamp){
path <- c(vStart)
while(length(path) < (nSteps+1)){
path <- c(path, sample(Edges[[tail(path,1)]],1))
if(tail(path,1)==vGoal){
MCounter[length(path)-1] <- MCounter[length(path)-1]+1
break
}else if(length(path)==(nSteps+1)){
MCounter[length(MCounter)] <- MCounter[length(MCounter)]+1
}
}
}
hitMat <- data.frame(vertex=vStart, steps=1:nSteps, goalprob=(MCounter/sum(MCounter))[-length(MCounter)])
return(hitMat)
}
# Function that return posterior mean for sampled paths over graph
MC.hitMat <- function(Edges, vStart, vGoal, nSteps, nSamp){
MCounter <- rep(0, nSteps+1)
for(r in 1:nSamp){
path <- c(vStart)
while(length(path) < (nSteps+1)){
path <- c(path, sample(Edges[[tail(path,1)]],1))
if(tail(path,1)==vGoal){
MCounter[length(path)-1] <- MCounter[length(path)-1]+1
break
}else if(length(path)==(nSteps+1)){
MCounter[length(MCounter)] <- MCounter[length(MCounter)]+1
}
}
}
hitMat <- data.frame(vertex=vStart, steps=1:nSteps, goalprob=(MCounter/sum(MCounter))[-length(MCounter)])
return(hitMat)
}
# Get the hitMat
hitMat <- foreach(v=inspect.Vertices, .combine=rbind) %do% {
tempMat <- MC.hitMat(Edges=Edges, vStart=v, vGoal=vGoal, nSteps=nSteps-1, nSamp=50000)
tempMat
}
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
piMat <- policy.generate(Edges=Edges, EVmat=EVmat, idmap=idmap)
# Run several agents on this task, all encompassing different heuristics
AG.dat <- data.frame(pp=0, trial=0, trRew=0, nSteps=0, endV=0, totRew=0, strat='init')
for(pp in 1:nPP){
AG.dat <- rbind(AG.dat, RandomStopper( Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, LazyWaiter(    Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, RandomLengther(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, OptimalStopper(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0, piMat=piMat))
}
AG.dat <- AG.dat[-1,]
AG.dat$strat <- droplevels(AG.dat$strat)
# Summarize agent simulations per participant
AG.dat.ppEval <- data.frame(pp=0, totRew=0, endV.p=0, strat='init')
for(pp in 1:nPP){
for(strat in levels(AG.dat$strat)){
totRew <- AG.dat[AG.dat$pp==pp & AG.dat$trial==max(AG.dat$trial) & AG.dat$strat==strat,]$totRew
endV.p <- sum(AG.dat[AG.dat$pp==pp & AG.dat$strat==strat,]$endV==vGoal)
AG.dat.ppEval <- rbind(AG.dat.ppEval, data.frame(pp=pp, totRew=totRew, endV.p=endV.p, strat=strat))
}
}
AG.dat.ppEval <- AG.dat.ppEval[-1,]
# Plot agent data!
ggplot(AG.dat.ppEval, aes(x=totRew, col=strat)) +
geom_density()
gRew   <- 15    # Reward upon reaching vGoal
sCost  <- 0.31 # Points detracted from accumulated reward for each taken step
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
piMat <- policy.generate(Edges=Edges, EVmat=EVmat, idmap=idmap)
piMat
EVmat
hitMat
sCost  <- 0.2 # Points detracted from accumulated reward for each taken step
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
piMat <- policy.generate(Edges=Edges, EVmat=EVmat, idmap=idmap)
piMat
EVmat
piMat
# Run several agents on this task, all encompassing different heuristics
AG.dat <- data.frame(pp=0, trial=0, trRew=0, nSteps=0, endV=0, totRew=0, strat='init')
for(pp in 1:nPP){
AG.dat <- rbind(AG.dat, RandomStopper( Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, LazyWaiter(    Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, RandomLengther(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, OptimalStopper(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0, piMat=piMat))
}
AG.dat <- AG.dat[-1,]
AG.dat$strat <- droplevels(AG.dat$strat)
# Summarize agent simulations per participant
AG.dat.ppEval <- data.frame(pp=0, totRew=0, endV.p=0, strat='init')
for(pp in 1:nPP){
for(strat in levels(AG.dat$strat)){
totRew <- AG.dat[AG.dat$pp==pp & AG.dat$trial==max(AG.dat$trial) & AG.dat$strat==strat,]$totRew
endV.p <- sum(AG.dat[AG.dat$pp==pp & AG.dat$strat==strat,]$endV==vGoal)
AG.dat.ppEval <- rbind(AG.dat.ppEval, data.frame(pp=pp, totRew=totRew, endV.p=endV.p, strat=strat))
}
}
AG.dat.ppEval <- AG.dat.ppEval[-1,]
# Plot agent data!
ggplot(AG.dat.ppEval, aes(x=totRew, col=strat)) +
geom_density()
# Load Functions from /src/
sapply(paste0('src/',list.files('src/')), source)
# Get different Defining Parameters
vStart <- 1    # Nr of starting node
vGoal  <- 8    # Nr of goal (terminating, rewarding) node
nSteps <- 15   # Nr of maximum steps in a miniblock (hitMat and EVcalc will use nSteps-1; agent simulations will use nSteps!)
gRew   <- 15    # Reward upon reaching vGoal
sCost  <- 0.2 # Points detracted from accumulated reward for each taken step
# Get parameters for agentic simulations
nPP <- 250
nTr <- 100
#Full Schapiro-style edge matrix
Edges <- list(c(2, 3, 4, 5),
c(1, 3, 4, 5),
c(1, 2, 4, 5),
c(1, 2, 3, 6),
c(1, 2, 3, 15),
c(4, 7, 8, 9),
c(6, 8, 9, 10),
c(6, 7, 9, 10),
c(6, 7, 8, 10),
c(11,7, 8, 9),
c(10,12,13,14),
c(11,13,14,15),
c(11,12,14,15),
c(11,12,13,15),
c(5, 12,13,14))
########## Write the analysis ----------
inspect.Vertices <- c(1,4,5,6,7)
# Group identical nodes
idmap <- list(a = c(1,2,3, 12,13,14), b = c(4,11), c = c(5,15), d = c(6,10), e = c(7,9))
# Get the hitMat
hitMat <- foreach(v=inspect.Vertices, .combine=rbind) %do% {
tempMat <- MC.hitMat(Edges=Edges, vStart=v, vGoal=vGoal, nSteps=nSteps-1, nSamp=100000)
tempMat
}
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
piMat <- policy.generate(Edges=Edges, EVmat=EVmat, idmap=idmap)
# Run several agents on this task, all encompassing different heuristics
AG.dat <- data.frame(pp=0, trial=0, trRew=0, nSteps=0, endV=0, totRew=0, strat='init')
for(pp in 1:nPP){
AG.dat <- rbind(AG.dat, RandomStopper( Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, LazyWaiter(    Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, RandomLengther(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, OptimalStopper(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0, piMat=piMat))
}
AG.dat <- AG.dat[-1,]
AG.dat$strat <- droplevels(AG.dat$strat)
# Summarize agent simulations per participant
AG.dat.ppEval <- data.frame(pp=0, totRew=0, endV.p=0, strat='init')
for(pp in 1:nPP){
for(strat in levels(AG.dat$strat)){
totRew <- AG.dat[AG.dat$pp==pp & AG.dat$trial==max(AG.dat$trial) & AG.dat$strat==strat,]$totRew
endV.p <- sum(AG.dat[AG.dat$pp==pp & AG.dat$strat==strat,]$endV==vGoal)
AG.dat.ppEval <- rbind(AG.dat.ppEval, data.frame(pp=pp, totRew=totRew, endV.p=endV.p, strat=strat))
}
}
AG.dat.ppEval <- AG.dat.ppEval[-1,]
# Plot agent data!
ggplot(AG.dat.ppEval, aes(x=totRew, col=strat)) +
geom_density()
hitMat
#Full Schapiro-style edge matrix
Edges <- list(c(2, 3, 4, 5),
c(1, 3, 4, 5),
c(1, 2, 4, 5),
c(1, 2, 3, 6),
c(1, 2, 3, 15),
c(4, 7, 8, 9),
c(6, 8, 9, 10),
c(6, 7, 9, 10),
c(6, 7, 8, 10),
c(7, 8, 9),
c(12,13,14),
c(11,13,14,15),
c(11,12,14,15),
c(11,12,13,15),
c(5, 12,13,14))
############################################################################################################################
####################### Test script for experiments using the Blocked graph with backtracking ##############################
############################################################################################################################
# Load Packages
library(ggplot2)
library(reshape2)
library(ggthemes)
library(RColorBrewer)
library(ggstance)
library(lemon)
library(foreach)
library(DirichletReg)
# Load Functions from /src/
sapply(paste0('src/',list.files('src/')), source)
# Get different Defining Parameters
vStart <- 1    # Nr of starting node
vGoal  <- 8    # Nr of goal (terminating, rewarding) node
nSteps <- 15   # Nr of maximum steps in a miniblock (hitMat and EVcalc will use nSteps-1; agent simulations will use nSteps!)
gRew   <- 15    # Reward upon reaching vGoal
sCost  <- 0.2 # Points detracted from accumulated reward for each taken step
# Get parameters for agentic simulations
nPP <- 250
nTr <- 100
#Full Schapiro-style edge matrix
Edges <- list(c(2, 3, 4, 5),
c(1, 3, 4, 5),
c(1, 2, 4, 5),
c(1, 2, 3, 6),
c(1, 2, 3, 15),
c(4, 7, 8, 9),
c(6, 8, 9, 10),
c(6, 7, 9, 10),
c(6, 7, 8, 10),
c(7, 8, 9),
c(12,13,14),
c(11,13,14,15),
c(11,12,14,15),
c(11,12,13,15),
c(5, 12,13,14))
inspect.Vertices <- c(1,4,5,6,7,10,11,12,15)
idmap <- list(a = c(1,2,3), b = c(4), c = c(5), d = c(6), e = c(7,9), f = c(10), g = c(11), h = c(12,13,14), i = c(15))
# Get the hitMat
hitMat <- foreach(v=inspect.Vertices, .combine=rbind) %do% {
tempMat <- MC.hitMat(Edges=Edges, vStart=v, vGoal=vGoal, nSteps=nSteps-1, nSamp=100000)
tempMat
}
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat <- foreach(v = inspect.Vertices, .combine=rbind) %do% {
tempMat <- EVcalc(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat, Vertex=v)
tempMat
}
EVmat
piMat <- policy.generate(Edges=Edges, EVmat=EVmat, idmap=idmap)
piMat
EVmat
piMat <- policy.generate(Edges=Edges, EVmat=EVmat, idmap=idmap)
# Run several agents on this task, all encompassing different heuristics
AG.dat <- data.frame(pp=0, trial=0, trRew=0, nSteps=0, endV=0, totRew=0, strat='init')
for(pp in 1:nPP){
AG.dat <- rbind(AG.dat, RandomStopper( Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, LazyWaiter(    Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, RandomLengther(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, OptimalStopper(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0, piMat=piMat))
}
AG.dat <- AG.dat[-1,]
AG.dat$strat <- droplevels(AG.dat$strat)
# Summarize agent simulations per participant
AG.dat.ppEval <- data.frame(pp=0, totRew=0, endV.p=0, strat='init')
for(pp in 1:nPP){
for(strat in levels(AG.dat$strat)){
totRew <- AG.dat[AG.dat$pp==pp & AG.dat$trial==max(AG.dat$trial) & AG.dat$strat==strat,]$totRew
endV.p <- sum(AG.dat[AG.dat$pp==pp & AG.dat$strat==strat,]$endV==vGoal)
AG.dat.ppEval <- rbind(AG.dat.ppEval, data.frame(pp=pp, totRew=totRew, endV.p=endV.p, strat=strat))
}
}
AG.dat.ppEval <- AG.dat.ppEval[-1,]
# Plot agent data!
ggplot(AG.dat.ppEval, aes(x=totRew, col=strat)) +
geom_density()
nTr <- 200
# Run several agents on this task, all encompassing different heuristics
AG.dat <- data.frame(pp=0, trial=0, trRew=0, nSteps=0, endV=0, totRew=0, strat='init')
for(pp in 1:nPP){
AG.dat <- rbind(AG.dat, RandomStopper( Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, LazyWaiter(    Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, RandomLengther(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, OptimalStopper(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0, piMat=piMat))
}
AG.dat <- AG.dat[-1,]
AG.dat$strat <- droplevels(AG.dat$strat)
# Summarize agent simulations per participant
AG.dat.ppEval <- data.frame(pp=0, totRew=0, endV.p=0, strat='init')
for(pp in 1:nPP){
for(strat in levels(AG.dat$strat)){
totRew <- AG.dat[AG.dat$pp==pp & AG.dat$trial==max(AG.dat$trial) & AG.dat$strat==strat,]$totRew
endV.p <- sum(AG.dat[AG.dat$pp==pp & AG.dat$strat==strat,]$endV==vGoal)
AG.dat.ppEval <- rbind(AG.dat.ppEval, data.frame(pp=pp, totRew=totRew, endV.p=endV.p, strat=strat))
}
}
AG.dat.ppEval <- AG.dat.ppEval[-1,]
# Plot agent data!
ggplot(AG.dat.ppEval, aes(x=totRew, col=strat)) +
geom_density()
############################################################################################################################
###################### Master Script for Modular Graphs showcasing all functions and workflow ##############################
############################################################################################################################
# Load Packages
library(ggplot2)
library(reshape2)
library(ggthemes)
library(RColorBrewer)
library(ggstance)
library(lemon)
library(foreach)
library(DirichletReg)
# Load Functions from /src/
sapply(paste0('src/',list.files('src/')), source)
# Get different Defining Parameters
vStart <- 1    # Nr of starting node
vGoal  <- 8    # Nr of goal (terminating, rewarding) node
nSteps <- 10   # Nr of maximum steps in a miniblock (hitMat and EVcalc will use nSteps-1; agent simulations will use nSteps!)
gRew   <- 7    # Reward upon reaching vGoal
sCost  <- 0.15 # Points detracted from accumulated reward for each taken step
# Get parameters for agentic simulations
nPP <- 250
nTr <- 100
#Full Schapiro-style edge matrix
Edges <- list(c(2, 3, 4, 5),
c(1, 3, 4, 5),
c(1, 2, 4, 5),
c(1, 2, 3, 6),
c(1, 2, 3, 15),
c(4, 7, 8, 9),
c(6, 8, 9, 10),
c(6, 7, 9, 10),
c(6, 7, 8, 10),
c(11,7, 8, 9),
c(10,12,13,14),
c(11,13,14,15),
c(11,12,14,15),
c(11,12,13,15),
c(5, 12,13,14))
############# No-BackTracking Schapiro Analysis ----------------------------
# Note: should in principle work for ANY graph which has an equal degree for each node!
inspect.Edges <- rbind(c(1,2), #List all pre- and current vertices to inspect
c(1,4),
c(1,5),
c(4,1),
c(4,6),
c(5,1),
c(5,15),
c(6,4),
c(6,7),
c(7,6),
c(7,9))
idmap <- list(a = c(1,2,3, 12,13,14), b = c(4,11), c = c(5,15), d = c(6,10), e = c(7,9))
# Get the hitMat
hitMat.nBT <- foreach(preV = inspect.Edges[,1], curV = inspect.Edges[,2], .combine=rbind) %do% {
tempMat <- hitMat.calc.nBT(Edges=Edges, vGoal=vGoal, nSteps=nSteps-1, curV=curV, preV=preV, totP=3^(nSteps-1))
tempMat
}
# Get the Expected Values for each interaction of previous & current node, conditional upon steps left
EVmat.nBT <- foreach(preV = inspect.Edges[,1], curV = inspect.Edges[,2], .combine=rbind) %do% {
tempMat <- EVcalc.nBT(Edges=Edges, curV=curV, preV=preV, vGoal=vGoal, nSteps=nSteps-1, gRew=gRew, sCost=sCost, hitMat=hitMat.nBT)
tempMat
}
# Get the optimal stopping index (equals or lower) for each possible transition
piMat.nBT <- policy.generate.nBT(Edges=Edges, EVmat=EVmat.nBT, idmap=idmap)
# Run several agents on this task, all encompassing different heuristics
AG.dat <- data.frame(pp=0, trial=0, trRew=0, nSteps=0, endV=0, totRew=0, strat='init')
for(pp in 1:nPP){
AG.dat <- rbind(AG.dat, RandomStopper.nBT( Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, LazyWaiter.nBT(    Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, RandomLengther.nBT(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0))
AG.dat <- rbind(AG.dat, OptimalStopper.nBT(Edges=Edges, vStart=vStart, vGoal=vGoal, nSteps=nSteps, gRew=gRew, sCost=sCost, nTrials=nTr, parNum=pp, startRew=0, piMat=piMat.nBT))
}
AG.dat <- AG.dat[-1,]
AG.dat$strat <- droplevels(AG.dat$strat)
# Summarize agent simulations per participant
AG.dat.ppEval <- data.frame(pp=0, totRew=0, endV.p=0, strat='init')
for(pp in 1:nPP){
for(strat in levels(AG.dat$strat)){
totRew <- AG.dat[AG.dat$pp==pp & AG.dat$trial==max(AG.dat$trial) & AG.dat$strat==strat,]$totRew
endV.p <- sum(AG.dat[AG.dat$pp==pp & AG.dat$strat==strat,]$endV==vGoal)
AG.dat.ppEval <- rbind(AG.dat.ppEval, data.frame(pp=pp, totRew=totRew, endV.p=endV.p, strat=strat))
}
}
AG.dat.ppEval <- AG.dat.ppEval[-1,]
# Plot agent data!
ggplot(AG.dat.ppEval, aes(x=totRew, col=strat)) +
geom_density()
